{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W207 Baseline Submission\n",
    "## Group 2: Austin Jin, Matt Lyons, Chandni Shah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Project Link:</b> https://www.kaggle.com/c/petfinder-pawpularity-score\n",
    "\n",
    "<b> Project Description:</b><br>\n",
    "\"In this competition, you’ll analyze raw images and metadata to predict the “Pawpularity” of pet photos. Your task is to predict engagement with a pet's profile based on the photograph for that profile. You are also provided with hand-labelled metadata for each photo. The dataset for this competition therefore comprises both images and tabular data.\" <br><br>\n",
    "\n",
    "\"Tabular Metadata: Each pet photo is labeled with the value of 1 (Yes) or 0 (No) for each of the following features. These labels are not used for deriving the Pawpularity score.\n",
    "\n",
    "- Focus - Pet stands out against uncluttered background, not too close / far.\n",
    "- Eyes - Both eyes are facing front or near-front, with at least 1 eye / pupil decently clear.\n",
    "- Face - Decently clear face, facing front or near-front.\n",
    "- Near - Single pet taking up significant portion of photo (roughly over 50% of photo width or height).\n",
    "- Action - Pet in the middle of an action (e.g., jumping).\n",
    "- Accessory - Accompanying physical or digital accessory / prop (i.e. toy, digital sticker), excluding collar and leash.\n",
    "- Group - More than 1 pet in the photo.\n",
    "- Collage - Digitally-retouched photo (i.e. with digital photo frame, combination of multiple photos).\n",
    "- Human - Human in the photo.\n",
    "- Occlusion - Specific undesirable objects blocking part of the pet (i.e. human, cage or fence). Note that not all blocking objects are considered occlusion.\n",
    "- Info - Custom-added text or labels (i.e. pet name, description).\n",
    "- Blur - Noticeably out of focus or noisy, especially for the pet’s eyes and face. For Blur entries, “Eyes” column is always set to 0.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I - Metadata EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to better understand the 'petfinder-pawpularity-score' dataset, we have performed some early EDA by performing the following pre-requisites:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load in the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T01:41:54.757739Z",
     "iopub.status.busy": "2021-11-17T01:41:54.757197Z",
     "iopub.status.idle": "2021-11-17T01:41:54.772070Z",
     "shell.execute_reply": "2021-11-17T01:41:54.771182Z",
     "shell.execute_reply.started": "2021-11-17T01:41:54.757701Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf\n",
    "from matplotlib import image\n",
    "from glob import glob\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from math import sqrt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T01:42:13.152107Z",
     "iopub.status.busy": "2021-11-17T01:42:13.151833Z",
     "iopub.status.idle": "2021-11-17T01:42:14.475136Z",
     "shell.execute_reply": "2021-11-17T01:42:14.474361Z",
     "shell.execute_reply.started": "2021-11-17T01:42:13.152077Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Perform a for loop to browse through the 'petfinder-pawpularity-score' directory and print out all file names:\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T01:42:15.328552Z",
     "iopub.status.busy": "2021-11-17T01:42:15.327977Z",
     "iopub.status.idle": "2021-11-17T01:42:15.390701Z",
     "shell.execute_reply": "2021-11-17T01:42:15.389902Z",
     "shell.execute_reply.started": "2021-11-17T01:42:15.328512Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the source path for the Pawpularity contest data, retrieve and assign the .csv metadata into DataFrames, and retrieve and assign the .jpq image data into lists:\n",
    "# path = '../input/petfinder-pawpularity-score/'\n",
    "\n",
    "train_df = pd.read_csv('./train.csv')\n",
    "test_df = pd.read_csv('./test.csv')\n",
    "\n",
    "train_jpg = glob(\"./train/*.jpg\")\n",
    "test_jpg = glob(\"./test/*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T01:42:17.783766Z",
     "iopub.status.busy": "2021-11-17T01:42:17.782902Z",
     "iopub.status.idle": "2021-11-17T01:42:17.790975Z",
     "shell.execute_reply": "2021-11-17T01:42:17.790298Z",
     "shell.execute_reply.started": "2021-11-17T01:42:17.783720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df dimensions:  (9912, 14)\n",
      "train_df column names:  ['Id', 'Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory', 'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur', 'Pawpularity']\n",
      "\n",
      "test_df dimensions:  (8, 13)\n",
      "test_df column names:  ['Id', 'Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory', 'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur']\n"
     ]
    }
   ],
   "source": [
    "# Printing the dimensions for the train metadata.\n",
    "print('train_df dimensions: ', train_df.shape)\n",
    "print('train_df column names: ', train_df.columns.values.tolist())\n",
    "\n",
    "# Adding a space in between the dimensions for the train and test metadata\n",
    "print('')\n",
    "\n",
    "# Printing the dimensions for the test metadata\n",
    "print('test_df dimensions: ',test_df.shape)\n",
    "print('test_df column names: ', test_df.columns.values.tolist())\n",
    "\n",
    "# After printing the shape of the train_df and test_df DataFrames, we have noticed that the train_df has 9912 rows and 14 columns whereas the test_df only has 8 rows and 13 columns. It is also worth mentioning that the test_df dataframe doesn't contain the pawpularity score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After printing the shape of the train_df and test_df dataframes, we can see that the train_df has 14 columns and 9912 rows, while the test_df only has 13 columns and 8 rows. It is also worth noting that the test_df particulary hasn't have pawpularity score data attached to it. We have further explored the metadata in the train dataframe since it would be the dataset for building out our models and have decided to utilize the test dataframe for practicing some predictions since it didn't contain a column for pawpularity score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T01:33:16.746636Z",
     "iopub.status.busy": "2021-11-17T01:33:16.745961Z",
     "iopub.status.idle": "2021-11-17T01:33:16.772042Z",
     "shell.execute_reply": "2021-11-17T01:33:16.771333Z",
     "shell.execute_reply.started": "2021-11-17T01:33:16.746597Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display the first 10 rows of the train_df dataframe\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It was noticed that train_df still contains ID's of the photos which means that we won't be necessarily using this metadata when building out the models. Since we figured that it would be useful to also take a look athe distribution of the target variable, which would be the Pawpularity Score in the ranges from 1-100, a simple histogram has been plotted out to view the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T01:33:19.524177Z",
     "iopub.status.busy": "2021-11-17T01:33:19.523465Z",
     "iopub.status.idle": "2021-11-17T01:33:20.053933Z",
     "shell.execute_reply": "2021-11-17T01:33:20.053193Z",
     "shell.execute_reply.started": "2021-11-17T01:33:19.524139Z"
    }
   },
   "outputs": [],
   "source": [
    "# Distribution for Pawpularity Scores\n",
    "\n",
    "sns.set(rc={'figure.figsize':(15,5)})\n",
    "fig = plt.figure()\n",
    "sns.histplot(data=train_df, x='Pawpularity', bins=100)\n",
    "plt.axvline(train_df['Pawpularity'].mean(), c='red', ls='-', lw=3, label='Mean Pawpularity')\n",
    "plt.axvline(train_df['Pawpularity'].median(),c='blue',ls='-',lw=3, label='Median Pawpularity')\n",
    "plt.title('Distribution of Pawpularity Scores', fontsize=20, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After taking a look at the histogram, we see that there is a skew in the distribution of the pawpularity scores. It was interesting to see that there is a small curve close to zero Pawpularity along with another curve at the 100 Pawpularity Score with a count of close to 300. Since the EDA alone doesn't allow us to truly know the reason as to why there were many scores at 100, we have decided to keep the following theories in mind:\n",
    "\n",
    "##### - Was there something unique about the animals such as their age, color, or breed that was most desirable by the people visiting the site?\n",
    "##### - Did it have to do with the way in which photos were taken that were leading to more clicks and thus a higher Pawpularity score?\n",
    "##### - Did it have to do with the Pawpularity score itself?\n",
    "##### - Were there any outliers that need to be removed from the training data to improve the models that were built?\n",
    "##### - Was there perhaps any noise in the dataset that caused the huge increase in pawpularity scores of 100?\n",
    "\n",
    "### Since we are unable to find the actual answer through EDA alone, we plan on looking to develop concrete ML models that will allow us to see which features make more impact on the high pawpularity score in order to further explain the curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T01:33:23.138636Z",
     "iopub.status.busy": "2021-11-17T01:33:23.138110Z",
     "iopub.status.idle": "2021-11-17T01:33:23.158782Z",
     "shell.execute_reply": "2021-11-17T01:33:23.158045Z",
     "shell.execute_reply.started": "2021-11-17T01:33:23.138599Z"
    }
   },
   "outputs": [],
   "source": [
    "# Describe the distribution of the train dataframe in a numerical way\n",
    "\n",
    "train_df[['Pawpularity']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T01:33:25.433946Z",
     "iopub.status.busy": "2021-11-17T01:33:25.431785Z",
     "iopub.status.idle": "2021-11-17T01:33:34.963747Z",
     "shell.execute_reply": "2021-11-17T01:33:34.962870Z",
     "shell.execute_reply.started": "2021-11-17T01:33:25.433909Z"
    }
   },
   "outputs": [],
   "source": [
    "# Put column names into a list\n",
    "feature_variables = train_df.columns.values.tolist()\n",
    "\n",
    "# For each feature variable, doesn't include Id and Pawpularity by using [1:-1]\n",
    "# Display a boxplot and distribution plot against pawpularity\n",
    "for variable in feature_variables[1:-1]:\n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    sns.boxplot(data=train_df, x=variable, y='Pawpularity', ax=ax[0])\n",
    "    sns.histplot(train_df, x=\"Pawpularity\", hue=variable, kde=True, ax=ax[1])\n",
    "    plt.suptitle(variable, fontsize=20, fontweight='bold')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As you can see from the charts, the distribution of pawpularity scores is very similar for each feature variable which means that changing the features doesn't end up influencing the pawpularity scores as much. This would mean that we would need to use the images and not the .csv metadata. This would've not been realized if it hadn't been for the EDA that was performed. We will focus on analyzing the pixels for the remainder of the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II - Pixel EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before resizing the images to a uniform size, we have decided to explore the image data by taking a look at the first image in the train_jpg dataset and plotting that initial image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T01:21:12.643404Z",
     "iopub.status.busy": "2021-11-17T01:21:12.642892Z",
     "iopub.status.idle": "2021-11-17T01:21:12.651122Z",
     "shell.execute_reply": "2021-11-17T01:21:12.650294Z",
     "shell.execute_reply.started": "2021-11-17T01:21:12.643369Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train_jpg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T01:21:18.206497Z",
     "iopub.status.busy": "2021-11-17T01:21:18.205650Z",
     "iopub.status.idle": "2021-11-17T01:21:18.471309Z",
     "shell.execute_reply": "2021-11-17T01:21:18.467471Z",
     "shell.execute_reply.started": "2021-11-17T01:21:18.206432Z"
    }
   },
   "outputs": [],
   "source": [
    "path_image = train_jpg[0]\n",
    "array_image = plt.imread(path_image) \n",
    "print(array_image.shape)\n",
    "\n",
    "plt.imshow(array_image)\n",
    "plt.title('Initial Training Image') \n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we have attached a Pawpularity score as the title next to each image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T01:21:34.085430Z",
     "iopub.status.busy": "2021-11-17T01:21:34.085100Z",
     "iopub.status.idle": "2021-11-17T01:21:34.989457Z",
     "shell.execute_reply": "2021-11-17T01:21:34.988750Z",
     "shell.execute_reply.started": "2021-11-17T01:21:34.085391Z"
    }
   },
   "outputs": [],
   "source": [
    "for x in range(3):\n",
    "    path_image = train_jpg[x]\n",
    "    array_image = plt.imread(path_image) \n",
    "    print(\"The image {}'s dimensions are: {}\".format(x,array_image.shape))\n",
    "    plt.imshow(array_image)\n",
    "    plt.title(x) \n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After gaining an initial sense of images looked, we have decided to start resizing the images to a uniform size. In this transformation, we also add white padding to images to help preserve image quality during the resizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T01:24:22.511090Z",
     "iopub.status.busy": "2021-11-17T01:24:22.510370Z",
     "iopub.status.idle": "2021-11-17T01:24:22.517461Z",
     "shell.execute_reply": "2021-11-17T01:24:22.516803Z",
     "shell.execute_reply.started": "2021-11-17T01:24:22.511040Z"
    }
   },
   "outputs": [],
   "source": [
    "## process in the training and test data, including the bw 1-d train data for baseline\n",
    "\n",
    "train_path = './train_resized'\n",
    "train_bw_path = './train_resized_bw'\n",
    "test_path = './test'\n",
    "\n",
    "train_jpg = glob(train_path + \"/*.jpg\")\n",
    "train_bw_jpg = glob(train_bw_path + \"/*.jpg\")\n",
    "test_jpg = glob(test_path + \"/*.jpg\")\n",
    "\n",
    "\n",
    "train_images = [cv2.imread(file) for file in train_jpg]\n",
    "train_bw_images_1d = [cv2.imread(file, 0).flatten(order = 'C') for file in train_bw_jpg] # 0 for grayscale, C for row-style flattening\n",
    "test_images = [cv2.imread(file) for file in test_jpg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T01:24:05.807985Z",
     "iopub.status.busy": "2021-11-17T01:24:05.807708Z",
     "iopub.status.idle": "2021-11-17T01:24:05.814207Z",
     "shell.execute_reply": "2021-11-17T01:24:05.811517Z",
     "shell.execute_reply.started": "2021-11-17T01:24:05.807954Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array(train_bw_images_1d)\n",
    "X = X / 255\n",
    "Y = np.array(train_df['Pawpularity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T01:23:58.309047Z",
     "iopub.status.busy": "2021-11-17T01:23:58.308771Z",
     "iopub.status.idle": "2021-11-17T01:23:58.314714Z",
     "shell.execute_reply": "2021-11-17T01:23:58.313938Z",
     "shell.execute_reply.started": "2021-11-17T01:23:58.309015Z"
    }
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of Transformed Images, Top Scoring Images, and Bottom Scoring Images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#examples of transformed images\n",
    "pltnum = 0\n",
    "plt.figure(figsize=(100,100))\n",
    "\n",
    "for i in range(3):\n",
    "    pltnum += 1\n",
    "    plt.subplot(1, 3, pltnum)\n",
    "    plt.imshow(X[i].reshape(300,300), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#examples of score = 100\n",
    "\n",
    "y_100 = np.where(Y == 100)\n",
    "\n",
    "\n",
    "pltnum = 0\n",
    "plt.figure(figsize=(100,100))\n",
    "\n",
    "for i in y_100[0][:3]:\n",
    "    pltnum += 1\n",
    "    plt.subplot(1, 3, pltnum)\n",
    "    plt.imshow(X[i].reshape(300,300), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of scores in 75th percentile\n",
    "y_75quant = np.where(Y == np.percentile(Y, 75))\n",
    "\n",
    "pltnum = 0\n",
    "plt.figure(figsize=(100,100))\n",
    "\n",
    "for i in y_75quant[0][:3]:\n",
    "    pltnum += 1\n",
    "    plt.subplot(1, 3, pltnum)\n",
    "    plt.imshow(X[i].reshape(300,300), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of scores in 25th percentile\n",
    "y_25quant = np.where(Y == np.percentile(Y, 25))\n",
    "\n",
    "pltnum = 0\n",
    "plt.figure(figsize=(100,100))\n",
    "\n",
    "for i in y_25quant[0][:3]:\n",
    "    pltnum += 1\n",
    "    plt.subplot(1, 3, pltnum)\n",
    "    plt.imshow(X[i].reshape(300,300), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III - Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T01:25:55.891475Z",
     "iopub.status.busy": "2021-11-17T01:25:55.891216Z",
     "iopub.status.idle": "2021-11-17T01:25:55.921125Z",
     "shell.execute_reply": "2021-11-17T01:25:55.919949Z",
     "shell.execute_reply.started": "2021-11-17T01:25:55.891447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7929, 90000)\n",
      "(7929,)\n",
      "(1983, 90000)\n",
      "(1983,)\n"
     ]
    }
   ],
   "source": [
    "train_bw_images_1d, test_data, train_labels, test_labels = train_test_split(X,Y, test_size = .2, random_state = 42)\n",
    "print(train_bw_images_1d.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Regression Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score RSME, especially where k = 9\n",
    "\n",
    "def KNN(k_values):\n",
    "    for val in k_values:\n",
    "        KNN_model = KNeighborsRegressor(n_neighbors=val)\n",
    "        KNN_model.fit(train_bw_images_1d, train_labels)\n",
    "        test_predict = KNN_model.predict(test_data)\n",
    "        print(\"For k = \", val, \", the RMSE is: \", sqrt(mean_squared_error(test_labels, test_predict)), \"\\n\")\n",
    "        \n",
    "k_values = [1, 5, 9, 11, 55, 175, 201, 301, 501]\n",
    "KNN(k_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We observe the KNN regression produces optimized results for lowest RMSE between 201-301 neighbors (k = 201, RMSE = 21.006 and k = 301, RMSE = 21.0111). However, we can see the optimized RMSE is only slightly lower than when 9 neighbors are used (k = 9, RMSE = 21.901). Therefore we determine the benefits of the slightly lower RMSE are not worth the computing power of 200+ neighbors for our model. We will move forward with the k=9 KNN Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To comment out LRM\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(train_bw_images_1d, train_labels)\n",
    "lr_model.intercept_, lr_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.score(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predict = lr_model.predict(test_data)\n",
    "print(\"LR RMSE is: \", sqrt(mean_squared_error(test_labels, lr_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can observe the linear regression model performs poorly compared to the KNN models. We achieve an RMSE score of 31.681, much higher than any of the KNN models. Separately, we achieve a negative R squared score, which means the model's best-fit line fits worse than a horizontal line.\n",
    "\n",
    "### Based on these results, we will move forward with the KNN Regression with our baseline model. The RMSE for our baseline model is 21.901.\n",
    "\n",
    "### Our next steps will be to build a CNN model, which we hope will be able to better handle the complexity of the images and, in return, lower the RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T01:42:30.745242Z",
     "iopub.status.busy": "2021-11-17T01:42:30.744970Z",
     "iopub.status.idle": "2021-11-17T01:42:30.758321Z",
     "shell.execute_reply": "2021-11-17T01:42:30.757451Z",
     "shell.execute_reply.started": "2021-11-17T01:42:30.745211Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setting the file path of each image\n",
    "\n",
    "train_df[\"path\"] = train_df[\"Id\"].apply(lambda x: \"./train/\" + x + \".jpg\")\n",
    "test_df[\"path\"] = test_df[\"Id\"].apply(lambda x: \"./test/\" + x + \".jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T01:42:32.746899Z",
     "iopub.status.busy": "2021-11-17T01:42:32.746610Z",
     "iopub.status.idle": "2021-11-17T01:42:32.764993Z",
     "shell.execute_reply": "2021-11-17T01:42:32.764104Z",
     "shell.execute_reply.started": "2021-11-17T01:42:32.746867Z"
    }
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 64\n",
    "IMG_SIZE = 224\n",
    "target = 'Pawpularity'\n",
    "seed = 0\n",
    "\n",
    "def set_seed(seed=seed):\n",
    "    \"\"\"Utility function to use for reproducibility.\n",
    "    :param seed: Random seed\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "\n",
    "def set_display():\n",
    "    \"\"\"Function sets display options for charts and pd.DataFrames.\n",
    "    \"\"\"\n",
    "    # Plots display settings\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    plt.rcParams['figure.figsize'] = 12, 8\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    # DataFrame display settings\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.options.display.float_format = '{:.4f}'.format\n",
    "\n",
    "\n",
    "def id_to_path(img_id: str, dir: str):\n",
    "    \"\"\"Function returns a path to an image file.\n",
    "    :param img_id: Image Id\n",
    "    :param dir: Path to the directory with images\n",
    "    :return: Image file path\n",
    "    \"\"\"\n",
    "    return os.path.join(dir, f'{img_id}.jpg')\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def get_image(path: str) -> tf.Tensor:\n",
    "    \"\"\"Function loads image from a file and preprocesses it.\n",
    "    :param path: Path to image file\n",
    "    :return: Tensor with preprocessed image\n",
    "    \"\"\"\n",
    "    print(f\"IMAGE PROCESSING {str}\")\n",
    "    ## Decoding the image\n",
    "    image = tf.image.decode_jpeg(tf.io.read_file(path), channels=3)\n",
    "\n",
    "    ## Resizing image\n",
    "    image = tf.cast(tf.image.resize_with_pad(image, IMG_SIZE, IMG_SIZE), dtype=tf.int32)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def process_dataset(path: str, label: int) -> tuple:\n",
    "    \"\"\"Function returns preprocessed image and label.\n",
    "    :param path: Path to image file\n",
    "    :param label: Class label\n",
    "    :return: tf.Tensor with preprocessed image, numeric label\n",
    "    \"\"\"\n",
    "    return get_image(path), label\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def get_dataset(x, y=None) -> tf.data.Dataset:\n",
    "    \"\"\"Function creates batched optimized dataset for the model\n",
    "    out of an array of file paths and (optionally) class labels.\n",
    "    :param x: Input data for the model (array of file paths)\n",
    "    :param y: Target values for the model (array of class indexes)\n",
    "    :return TensorFlow Dataset object\n",
    "    \"\"\"\n",
    "    if y is not None:\n",
    "        ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "        return ds.map(process_dataset, num_parallel_calls=AUTOTUNE) \\\n",
    "            .batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
    "    else:\n",
    "        ds = tf.data.Dataset.from_tensor_slices(x)\n",
    "        return ds.map(get_image, num_parallel_calls=AUTOTUNE) \\\n",
    "            .batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "def plot_history(hist):\n",
    "    \"\"\"Function plots a chart with training and validation metrics.\n",
    "    :param hist: Tensorflow history object from model.fit()\n",
    "    \"\"\"\n",
    "    # Losses and metrics\n",
    "    loss = hist.history['loss']\n",
    "    val_loss = hist.history['val_loss']\n",
    "    rmse = hist.history['root_mean_squared_error']\n",
    "    val_rmse = hist.history['val_root_mean_squared_error']\n",
    "\n",
    "    # Epochs to plot along x axis\n",
    "    x_axis = range(1, len(loss) + 1)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True)\n",
    "\n",
    "    ax1.plot(x_axis, loss, 'bo', label='Training')\n",
    "    ax1.plot(x_axis, val_loss, 'ro', label='Validation', alpha=0.3)\n",
    "    ax1.set_title('MSE Loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(x_axis, rmse, 'bo', label='Training')\n",
    "    ax2.plot(x_axis, val_rmse, 'ro', label='Validation', alpha=0.3)\n",
    "    ax2.set_title('Root Mean Squared Error')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T01:42:36.813051Z",
     "iopub.status.busy": "2021-11-17T01:42:36.812330Z",
     "iopub.status.idle": "2021-11-17T01:42:36.822698Z",
     "shell.execute_reply": "2021-11-17T01:42:36.821882Z",
     "shell.execute_reply.started": "2021-11-17T01:42:36.813011Z"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting train into train and validation sets\n",
    "\n",
    "train_subset, valid_subset = train_test_split(\n",
    "    train_df[['path', target]],\n",
    "    test_size=.2, shuffle=True, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T01:43:16.932629Z",
     "iopub.status.busy": "2021-11-17T01:43:16.931983Z",
     "iopub.status.idle": "2021-11-17T01:43:16.944990Z",
     "shell.execute_reply": "2021-11-17T01:43:16.944062Z",
     "shell.execute_reply.started": "2021-11-17T01:43:16.932579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGE PROCESSING <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "train_ds = get_dataset(x=train_subset['path'], y=train_subset[target])\n",
    "valid_ds = get_dataset(x=valid_subset['path'], y=valid_subset[target])\n",
    "test_ds = get_dataset(x=test_df['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T01:43:21.875737Z",
     "iopub.status.busy": "2021-11-17T01:43:21.875185Z",
     "iopub.status.idle": "2021-11-17T01:43:21.888089Z",
     "shell.execute_reply": "2021-11-17T01:43:21.887201Z",
     "shell.execute_reply.started": "2021-11-17T01:43:21.875697Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "\n",
    "def get_model():\n",
    "    \n",
    "    ## Setting the Inputs\n",
    "    inputs = keras.Input(shape=(224, 224, 3))\n",
    "    x = inputs\n",
    "    \n",
    "    ## Preprocessing Layers\n",
    "    \n",
    "    ### Rescaling\n",
    "    x = keras.layers.experimental.preprocessing.Rescaling(1./255)(x)\n",
    "    \n",
    "    ## Data Augmentation\n",
    "    x = keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\")(x)\n",
    "    x = keras.layers.experimental.preprocessing.RandomRotation(0.2)(x)\n",
    "    x = keras.layers.experimental.preprocessing.RandomTranslation(0.2,0.2)(x)\n",
    "    \n",
    "    ## Convolutional Layers\n",
    "    \n",
    "    ### First CNN layer\n",
    "    x = keras.layers.Conv2D(filters=96, kernel_size=3, strides=2, padding='same', kernel_initializer=tf.keras.initializers.HeNormal())(x)\n",
    "    x = keras.layers.Activation('relu')(x)\n",
    "    x = keras.layers.MaxPool2D(2)(x)\n",
    "\n",
    "    ### Second CNN layer\n",
    "    x = keras.layers.Conv2D(filters=128, kernel_size=3, strides=2, padding='same', kernel_initializer=tf.keras.initializers.HeNormal())(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation('relu')(x)\n",
    "    x = keras.layers.MaxPool2D(2)(x)\n",
    "    \n",
    "    ### Third CNN layer\n",
    "    x = keras.layers.Conv2D(filters=256, kernel_size=3, strides=2, padding='same', kernel_initializer=tf.keras.initializers.HeNormal())(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation('relu')(x)\n",
    "    x = keras.layers.MaxPool2D(2)(x)\n",
    "\n",
    "    ## Flattening the layer\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    \n",
    "    ## Fully Connected (Dense) Layers\n",
    "    \n",
    "    ### First Fully Connected layer w/ Dropout\n",
    "    x = keras.layers.Dense(128, activation='relu', kernel_initializer=tf.keras.initializers.HeNormal())(x)\n",
    "    x = keras.layers.Dropout(0.2)(x)\n",
    "    \n",
    "    ## Output layer\n",
    "    output = keras.layers.Dense(1)(x)\n",
    "\n",
    "    ## Returning the model\n",
    "    return keras.Model(inputs=inputs, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T01:43:24.907149Z",
     "iopub.status.busy": "2021-11-17T01:43:24.906608Z",
     "iopub.status.idle": "2021-11-17T01:43:24.914961Z",
     "shell.execute_reply": "2021-11-17T01:43:24.914242Z",
     "shell.execute_reply.started": "2021-11-17T01:43:24.907109Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "\n",
    "def compile_and_fit(model):\n",
    "    \n",
    "    # Creating an exponential decay for learning rate\n",
    "\n",
    "    LEARNING_RATE = 1e-3\n",
    "    DECAY_STEPS = 100\n",
    "    DECAY_RATE = 0.99\n",
    "\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=LEARNING_RATE,\n",
    "        decay_steps=DECAY_STEPS, decay_rate=DECAY_RATE,\n",
    "        staircase=True\n",
    "    )\n",
    "    \n",
    "    # Creating an early stopper\n",
    "\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=5, restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_ds, \n",
    "        validation_data=valid_ds,\n",
    "        epochs=50,\n",
    "        use_multiprocessing=True, workers=-1,\n",
    "        callbacks=[early_stop]\n",
    "    )\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T01:43:31.313266Z",
     "iopub.status.busy": "2021-11-17T01:43:31.312604Z",
     "iopub.status.idle": "2021-11-17T01:43:31.545847Z",
     "shell.execute_reply": "2021-11-17T01:43:31.545078Z",
     "shell.execute_reply.started": "2021-11-17T01:43:31.313228Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0974ecce1771>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Getting the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "# Getting the model\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T01:43:40.137119Z",
     "iopub.status.busy": "2021-11-17T01:43:40.136842Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model\n",
    "\n",
    "model, history = compile_and_fit(model)\n",
    "# predictions = model.predict(valid_ds, use_multiprocessing=True, workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting accuracy and loss of model\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the model to predict on the test data\n",
    "\n",
    "test_df[target] = model.predict(\n",
    "    test_ds, use_multiprocessing=True, workers=os.cpu_count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the submission file\n",
    "\n",
    "test_df[['Id', target]].to_csv('submission.csv', index=False)\n",
    "test_df[['Id', target]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binning columns to test models\n",
    "train['two_bin_pawp'] = pd.qcut(train_df['Pawpularity'], q=2, labels=False)\n",
    "train = train.astype({\"two_bin_pawp\": str})\n",
    "\n",
    "train['four_bin_pawp'] = pd.qcut(train_df['Pawpularity'], q=4, labels=False)\n",
    "train = train.astype({\"four_bin_pawp\": str})\n",
    "\n",
    "train['ten_bin_pawp'] = pd.qcut(train_df['Pawpularity'], q=10, labels=False)\n",
    "train = train.astype({\"ten_bin_pawp\": str})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
